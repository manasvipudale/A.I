{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "803FJP_kVM--"
      },
      "outputs": [],
      "source": [
        "#Assignment no: 4\n",
        "#Name: Manasvi Pudale\n",
        "#Roll no: 3352\n",
        "#Problem Statement: Write a program for the information Retrieval System using appropriate NLP tools (such as NLTK, Open NLP,..)\n",
        "# a. Text Tokenization\n",
        "# b. Count word frequency\n",
        "# c. Remove stop words\n",
        "# d. POS tagging\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import random"
      ],
      "metadata": {
        "id": "JL22KXI-XCRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_OcJqVmWKj1",
        "outputId": "935da69b-6d16-4fc3-b795-7d81a44a0309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the text to be analyzed\n",
        "text=input(\"enter your text:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMw7RheQYMln",
        "outputId": "fa5293e3-52b1-4969-8027-e0f2edd0d8c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter your text:This is a collab notebook. and this assignment is based on natural language processing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize the words into words\n",
        "words=word_tokenize(text)\n",
        "print(\"tokenized words:\")\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0xj7zcEY15e",
        "outputId": "9b8a92b6-09fa-4946-ef52-184a1afd6a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized words:\n",
            "['This', 'is', 'a', 'collab', 'notebook', '.', 'and', 'this', 'assignment', 'is', 'based', 'on', 'natural', 'language', 'processing', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert all words to lowercase words\n",
        "[word.lower() for word in words]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nON7PWzUYllp",
        "outputId": "85a2ec6e-d52f-478a-edee-4a69b539d57b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'is',\n",
              " 'a',\n",
              " 'collab',\n",
              " 'notebook',\n",
              " '.',\n",
              " 'and',\n",
              " 'this',\n",
              " 'assignment',\n",
              " 'is',\n",
              " 'based',\n",
              " 'on',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the frequency of each word\n",
        "fdist=FreqDist(words)\n",
        "print(\"Word Frequency:\")\n",
        "for word,freq in fdist.items():\n",
        " print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woecaXWEZFPw",
        "outputId": "515c8aea-467c-4d88-df5a-4e8b57562213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Frequency:\n",
            "This: 1\n",
            "is: 2\n",
            "a: 1\n",
            "collab: 1\n",
            "notebook: 1\n",
            ".: 2\n",
            "and: 1\n",
            "this: 1\n",
            "assignment: 1\n",
            "based: 1\n",
            "on: 1\n",
            "natural: 1\n",
            "language: 1\n",
            "processing: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words=[word for word in words if word.casefold() not in stop_words]\n",
        "print(\"Filtered words:\")\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpKOIFNhZhqL",
        "outputId": "cbc34c4f-5022-4752-8673-2e1d991baeb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered words:\n",
            "['collab', 'notebook', '.', 'assignment', 'based', 'natural', 'language', 'processing', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#POS TAgging\n",
        "pos_tags=pos_tag(words)\n",
        "print(\"POS Tags:\")\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAsKXGc7dS_q",
        "outputId": "c136faba-e808-4a4f-8e53-6b59d2ea957f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags:\n",
            "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('collab', 'NN'), ('notebook', 'NN'), ('.', '.'), ('and', 'CC'), ('this', 'DT'), ('assignment', 'NN'), ('is', 'VBZ'), ('based', 'VBN'), ('on', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    }
  ]
}